## Анализ лога веб-сервера

### Цель:

Потренироваться писать парсеры для логов

### Описание:

Написать скрипт анализа [access.log](access.tar.gz) файла

Формат записи в файле лога:

```bash
%h - - %t "%r" %s %b "%{Referer}" "%{User-Agent}" %d
```

`%h` - IP-адрес удаленного хоста  
`%t` - время получения запроса  
`%r` - тип запроса, его содержимое и версия протокола  
`%s` - код состояния HTTP  
`%b` - количество отданных сервером байт  
`%{Referer}` - URL-источник запроса  
`%{User-Agent}` - HTTP-заголовок, содержащий информацию о запросе  
`%d` - длительность запроса в миллисекундах  

Требования к реализации:

1. Должна быть возможность указать директорий где искать логи или конкретный файл
2. Должна быть возможность обработки всех логов внутри одного директория
3. Для лога должна собираться следующая информация:
   - общее количество выполненных запросов
   - количество запросов по HTTP-методам: GET, POST, PUT, DELETE, OPTIONS, HEAD. Например, GET - 20, POST - 10 и т.п.
   - топ 3 IP адресов, с которых было сделано наибольшее количество запросов
   - топ 3 самых долгих запросов - должны выводиться HTTP-метод, URL, IP, длительность запроса, дата и время запроса
4. Собранная статистика должна быть сохранена в json-файл и выведена в терминал в следующем [формате](result.json)
5. Если в процессе работы скрипта было проанализировано несколько лог-файлов, то для каждого из файлов должен быть 
сформирован отдельный json-файл со статистикой и отдельный вывод в терминал
6. В репозитории должен присутствовать `README.md` файл, который описывает как работает скрипт

### Критерии оценки:

Статус "Принято" ставится, если:

1. Приложен скриншот с примером работы скрипта
2. Приложен json-файл с результатом обработки [access.log](access.tar.gz)
3. Задание выполнено и сдаётся в формате pull-request
4. Соблюдается минимальный код-стайл
5. В pull-request не попали никакие лишние изменения, которые не связаны с заданием

### Рекомендации:

Первое, что нужно сделать - это распаковать архив с файлом лога:

```shell
tar -xzvf access.tar.gz
```

Для того чтобы определить директорий или конкретный файл, был передан скрипту - можно использовать следующие методы:
- [os.path.isfile](https://docs.python.org/3/library/os.path.html#os.path.isfile)
- [os.path.isdir](https://docs.python.org/3/library/os.path.html#os.path.isdir)

Если скрипту был передан директорий - можно получить список всех файлов с помощью метода:
- [os.listdir](https://docs.python.org/3/library/os.html#os.listdir)

При этом стоит учесть, что [os.listdir](https://docs.python.org/3/library/os.html#os.listdir) возвращает список имён файлов.
Таким образом полные пути до файлов с логами нужно составить из пути до директория с файлами логов и полученных имён файлов.

Так как файл [access.log](access.tar.gz) достаточно большой (3216723 строк) - нужно собрать всю статистику за один проход по файлу.

Общее количество запросов в файле [access.log](access.tar.gz) можно сверить с выводом команды `wc -l`

```shell
wc -l access.log 
3216723 access.log
```

Количество запросов по HTTP-методам можно сверить с выводом команды `grep -c`. 
Например, чтобы определить количество GET-запросов в файле лога можно выполнить следующую команду:

```shell
grep -c '"GET ' access.log 
2247848
```

Топ 3 IP адресов с которых было сделано наибольшее количество запросов можно получить объединением в pipeline утилит, 
о которых мы говорили на лекции:

```shell
awk '{print $1}' access.log | sort | uniq -c | sort -r -s -n -k 1,1 | head -3
10 1.1.1.1
 9 2.2.2.2
 8 3.3.3.3
```

Здесь с помощью `awk` мы получаем список IP-адресов из файла лога. Далее с помощью `sort` сортируем этот список и с помощью
`uniq -с` получаем количество вхождений каждого IP-адреса в отсортированный список. Затем с помощью `sort` мы сортируем полученную 
статистику по убыванию количества вхождений и c помощью `head` выводим первые 3 результата.

Топ 3 самых долгих запросов можно получить используя комбинацию описанного ранее подхода с утилитой `grep`.
С помощью следующей команды мы получаем время исполнения самого долгого запроса:

```shell
awk '{print $NF}' access.log | sort | uniq | sort -r -s -n -k 1,1 | head -1
10000
```

Теперь мы можем найти 3 первых запроса с такой длительностью:

```shell
grep '10000$' access.log | head -3
1.1.1.1 - - [23/Dec/2015:07:27:57 +0100] "POST /administrator/index.php HTTP/1.1" 200 4494 "-" "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/23.0.1271.17 Safari/537.11" 10000
2.2.2.2 - - [06/Jan/2016:18:47:57 +0100] "GET /media/system/css/modal.css HTTP/1.1" 200 1159 "http://www.almhuette-raith.at/index.php?option=com_phocagallery&view=category&id=1&Itemid=53" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9" 10000
3.3.3.3 - - [06/Jan/2016:21:29:02 +0100] "GET /images/phocagallery/almhuette/thumbs/phoca_thumb_m_terasse.jpg HTTP/1.1" 200 5126 "http://www.almhuette-raith.at/index.php?option=com_phocagallery&view=category&id=1&Itemid=53" "Mozilla/5.0 (iPhone; CPU iPhone OS 9_2 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Version/9.0 Mobile/13C75 Safari/601.1" 10000
```

Команды приведённые выше нужно использовать исключительно для проверки работы вашего скрипта. 
В коде скрипта нельзя вызывать и использовать эти команды.